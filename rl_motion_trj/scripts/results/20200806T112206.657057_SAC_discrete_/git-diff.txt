diff --git a/rl_manipulator.zip b/rl_manipulator.zip
deleted file mode 100644
index c8f9750..0000000
Binary files a/rl_manipulator.zip and /dev/null differ
diff --git a/rl_motion_trj/scripts/rl_motion_trj_train.py b/rl_motion_trj/scripts/rl_motion_trj_train.py
index 7c730e9..cd124a0 100755
--- a/rl_motion_trj/scripts/rl_motion_trj_train.py
+++ b/rl_motion_trj/scripts/rl_motion_trj_train.py
@@ -1,13 +1,12 @@
 import gym
 
-from tf2rl.algos.sac import SAC
+from tf2rl.algos.sac_discrete import SACDiscrete
 from tf2rl.experiments.trainer import Trainer
 
-
 if __name__ == '__main__':
     parser = Trainer.get_argument()
-    parser = SAC.get_argument(parser)
-    parser.add_argument('--env-name', type=str, default="Pendulum-v0")
+    parser = SACDiscrete.get_argument(parser)
+    parser.add_argument('--env-name', type=str, default="CartPole-v0")
     parser.set_defaults(batch_size=100)
     parser.set_defaults(n_warmup=10000)
     parser.set_defaults(max_steps=3e6)
@@ -15,15 +14,17 @@ if __name__ == '__main__':
 
     env = gym.make(args.env_name)
     test_env = gym.make(args.env_name)
-    policy = SAC(
+
+    policy = SACDiscrete(
         state_shape=env.observation_space.shape,
-        action_dim=env.action_space.high.size,
-        gpu=args.gpu,
+        action_dim=1,
+        gpu=-1,
         memory_capacity=args.memory_capacity,
-        max_action=env.action_space.high[0],
+        max_action=1,
         batch_size=args.batch_size,
         n_warmup=args.n_warmup,
         alpha=args.alpha,
         auto_alpha=args.auto_alpha)
     trainer = Trainer(policy, env, args, test_env=test_env)
+
     trainer()
\ No newline at end of file
diff --git a/rl_motion_trj/scripts/sac_pendulum.zip b/rl_motion_trj/scripts/sac_pendulum.zip
deleted file mode 100644
index 0b87f0c..0000000
Binary files a/rl_motion_trj/scripts/sac_pendulum.zip and /dev/null differ
