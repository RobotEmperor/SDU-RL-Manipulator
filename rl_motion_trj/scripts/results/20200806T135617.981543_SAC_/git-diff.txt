diff --git a/.idea/vcs.xml b/.idea/vcs.xml
index 94a25f7..75d43de 100644
--- a/.idea/vcs.xml
+++ b/.idea/vcs.xml
@@ -2,5 +2,6 @@
 <project version="4">
   <component name="VcsDirectoryMappings">
     <mapping directory="$PROJECT_DIR$" vcs="Git" />
+    <mapping directory="$PROJECT_DIR$/soft-actor-critic" vcs="Git" />
   </component>
 </project>
\ No newline at end of file
diff --git a/rl_manipulator.zip b/rl_manipulator.zip
deleted file mode 100644
index c8f9750..0000000
Binary files a/rl_manipulator.zip and /dev/null differ
diff --git a/rl_motion_trj/scripts/rl_motion_trj_train.py b/rl_motion_trj/scripts/rl_motion_trj_train.py
index 7c730e9..b7d25bc 100755
--- a/rl_motion_trj/scripts/rl_motion_trj_train.py
+++ b/rl_motion_trj/scripts/rl_motion_trj_train.py
@@ -1,12 +1,12 @@
 import gym
 
-from tf2rl.algos.sac import SAC
+from tf2rl.algos.sac_discrete import SAC
 from tf2rl.experiments.trainer import Trainer
 
-
 if __name__ == '__main__':
     parser = Trainer.get_argument()
     parser = SAC.get_argument(parser)
+    #parser.add_argument('--env-name', type=str, default="CartPole-v0")
     parser.add_argument('--env-name', type=str, default="Pendulum-v0")
     parser.set_defaults(batch_size=100)
     parser.set_defaults(n_warmup=10000)
@@ -15,10 +15,11 @@ if __name__ == '__main__':
 
     env = gym.make(args.env_name)
     test_env = gym.make(args.env_name)
+
     policy = SAC(
         state_shape=env.observation_space.shape,
         action_dim=env.action_space.high.size,
-        gpu=args.gpu,
+        gpu=-1,
         memory_capacity=args.memory_capacity,
         max_action=env.action_space.high[0],
         batch_size=args.batch_size,
@@ -26,4 +27,5 @@ if __name__ == '__main__':
         alpha=args.alpha,
         auto_alpha=args.auto_alpha)
     trainer = Trainer(policy, env, args, test_env=test_env)
+
     trainer()
\ No newline at end of file
diff --git a/rl_motion_trj/scripts/sac_pendulum.zip b/rl_motion_trj/scripts/sac_pendulum.zip
deleted file mode 100644
index 0b87f0c..0000000
Binary files a/rl_motion_trj/scripts/sac_pendulum.zip and /dev/null differ
diff --git a/rl_motion_trj/scripts/simulation_test.py b/rl_motion_trj/scripts/simulation_test.py
index 216b95c..b08f2dd 100755
--- a/rl_motion_trj/scripts/simulation_test.py
+++ b/rl_motion_trj/scripts/simulation_test.py
@@ -1,33 +1,59 @@
-import unittest
-
+import numpy as np
+import gym
 from tf2rl.algos.sac import SAC
-from tests.algos.common import CommonOffPolContinuousAlgos
+from tf2rl.experiments.trainer import Trainer
+
+parser = Trainer.get_argument()
+parser = SAC.get_argument(parser)
+
+parser.add_argument('--seed', type=int, default=42,
+                    help='random seed')
+parser.add_argument('--env_name', type=str, default='Pendulum-v0',
+                    help='name of the gym environment with version')
+parser.add_argument('--render', type=bool, default=False,
+                    help='set gym environment to render display')
+parser.add_argument('--verbose', type=bool, default=False,
+                    help='log execution details')
+parser.add_argument('--model_path', type=str, default='../results',
+                    help='path to save model')
+parser.add_argument('--model_name', type=str,
+                    default=f' ',
+                    help='name of the saved model')
+while True:
+
+    args = parser.parse_args()
+
+    # Instantiate the environment.
+    env = gym.make(args.env_name)
+    env.seed(args.seed)
+    state_space = env.observation_space.shape[0]
+    # TODO: fix this when env.action_space is not `Box`
+    action_space = env.action_space.shape[0]
+
+    actor = SAC(action_space)
+
+    actor.load_weights(args.model_path + args.model_name + '/ckpt-1')
+
+    # Observe state
+    current_state = env.reset()
 
+    episode_reward = 0
+    done = False
+    while not done:
 
-class TestSAC(CommonOffPolContinuousAlgos):
-    @classmethod
-    def setUpClass(cls):
-        super().setUpClass()
-        cls.agent = SAC(
-            state_shape=cls.continuous_env.observation_space.shape,
-            action_dim=cls.continuous_env.action_space.low.size,
-            batch_size=cls.batch_size,
-            gpu=-1)
+        if args.render:
+            env.render()
 
+        current_state_ = np.array(current_state, ndmin=2)
+        action_, _ = actor(current_state_)
+        action = action_.numpy()[0]
 
-class TestSACAutoAlpha(CommonOffPolContinuousAlgos):
-    # TODO: Skip duplicated tests called in TestSAC
-    @classmethod
-    def setUpClass(cls):
-        super().setUpClass()
-        cls.agent = SAC(
-            state_shape=cls.continuous_env.observation_space.shape,
-            action_dim=cls.continuous_env.action_space.low.size,
-            batch_size=cls.batch_size,
-            auto_alpha=True,
-            gpu=-1)
+        # Execute action, observe next state and reward
+        next_state, reward, done, _ = env.step(action)
 
+        episode_reward +=  reward
 
+        # Update current state
+        current_state = next_state
 
-if __name__ == '__main__':
-    unittest.main()
\ No newline at end of file
+    print(episode_reward)
